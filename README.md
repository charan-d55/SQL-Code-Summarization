# SQL-Code-Summarization
Used the Llama-2-13B-chat-GGml model for the task.
Llama 2 is a set of pre-trained and fine-tuned text models optimized for dialogue tasks, ranging from 7 billion to 70 billion parameters. It surpasses open-source chat models on most metrics and matches closed-source models in human evaluations for usefulness and safety. Llama.cpp is a C/C++ implementation designed to run the LLaMA model with 4-bit integer quantization on MacBook, supporting various architectures and libraries. GGML, a C library for machine learning, aids in distributing large language models (LLMs) by enabling efficient execution on consumer hardware through quantization. The Hugging Face community offers quantized models, including variations based on the GGLM library, like Llama-2-13B-chat-GGML.
# Dataset Used
The SQL queries utilized in my study were sourced from the WikiSQL dataset. This dataset encompasses a collection of 87,726meticulously annotated SQL query and corresponding natural language question pairs. The queries are segmented into distinct sets for training (comprising 61,297 examples), development (with 9,145 examples), and testing (totaling 17,284 examples). This resource is particularly valuable for tasks involving natural language inference within relational databases. Specifically, I exclusively employed the test subset of queries to provide input for the Llama-2 model, thereby obtaining the model-generated outputs.
# Running Guide
I've included the Google Colab notebook (i.e., .ipynb file) in the repository. Simply run the file step by step. Additionally, I've thoroughly commented the notebook, providing guidance for each step. It's worth noting that I've upgraded to a Google Colab Pro subscription to accommodate higher GPU needs. If you're accessing the notebook, please consider using the Colab Pro version for optimal performance.
